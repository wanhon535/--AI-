# test_learning_loop.py

import os
import sys
import random

# --- 1. é¡¹ç›®ç¯å¢ƒè®¾ç½® ---
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

# --- 2. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å— ---
from src.model.lottery_models import LotteryHistory
from src.engine.performance_logger import PerformanceLogger
from src.algorithms.dynamic_ensemble_optimizer import DynamicEnsembleOptimizer
from src.database.database_manager import DatabaseManager
from src.engine.adaptive_weight_updater import AdaptiveWeightUpdater
from src.database.crud.algorithm_performance_dao import AlgorithmPerformanceDAO
# å¯¼å…¥åŸºç¡€ç®—æ³•ç”¨äºæµ‹è¯•
from src.algorithms.statistical_algorithms import FrequencyAnalysisAlgorithm
from src.algorithms.advanced_algorithms.bayesian_number_predictor import BayesianNumberPredictor


# --- 3. æ¨¡æ‹Ÿä¸€ä¸ªæ•°æ®åº“ (å…³é”®ï¼)**
# æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡çš„ DAO ç±»ï¼Œå®ƒæŠŠæ•°æ®å­˜åœ¨å†…å­˜é‡Œï¼Œè€Œä¸æ˜¯çœŸå®æ•°æ®åº“é‡Œ
class MockAlgorithmPerformanceDAO:
    def __init__(self, db_manager: DatabaseManager = None, dao=None, smoothing_alpha: float = 0.6,
                 hist_window: int = 5):
        """
        :param db_manager: ä¼ å…¥ä¸€ä¸ªå·²ç»é…ç½®å¥½çš„ DatabaseManager å®ä¾‹ (for production).
        :param dao: å¯é€‰ã€‚ä¼ å…¥ä¸€ä¸ªDAOå®ä¾‹ (for testing).
        :param smoothing_alpha: å¹³æ»‘ç³»æ•°ã€‚
        :param hist_window: å†å²çª—å£å¤§å°ã€‚
        """
        if dao:
            # Test mode: use the provided mock dao
            self.dao = dao
            print("[PerformanceLogger] Initialized in TEST mode with a mock DAO.")
        elif db_manager:
            # Production mode: create a real DAO using the db_manager
            self.dao = AlgorithmPerformanceDAO(db_manager)
            print("[PerformanceLogger] Initialized in PRODUCTION mode with a real DAO.")
        else:
            raise ValueError("PerformanceLogger requires either a 'db_manager' or a 'dao' instance.")

        self.updater = AdaptiveWeightUpdater(alpha=smoothing_alpha)
        self.hist_window = hist_window

    def get_average_scores_last_n_issues(self, n_issues: int = 3) -> dict:
        # ä»å†…å­˜ä¸­è®¡ç®—å¹³å‡åˆ†
        latest_issues = sorted(list(set(d['issue'] for d in self.db)), reverse=True)[:n_issues]
        if not latest_issues:
            print(" MOCK_DB: No historical scores found.")
            return {}

        scores = {}
        for issue in latest_issues:
            for record in self.db:
                if record['issue'] == issue:
                    scores.setdefault(record['algorithm'], []).append(record['score'])

        avg_scores = {algo: sum(s_list) / len(s_list) for algo, s_list in scores.items()}
        print(f" MOCK_DB: Calculated average scores from last {len(latest_issues)} issues: {avg_scores}")
        return avg_scores


def run_unit_test():
    print("\n" + "=" * 60)
    print("ğŸš€  STARTING UNIT TEST FOR THE SELF-LEARNING LOOP")
    print("=" * 60)

    # --- 1. å‡†å¤‡æ¨¡æ‹Ÿç¯å¢ƒ ---
    # ä½¿ç”¨å‡çš„æ•°æ®åº“DAO
    mock_dao = MockAlgorithmPerformanceDAO()

    # vvvvvvvvvvv  è¿™æ˜¯å…³é”®çš„ä¿®æ­£ï¼ vvvvvvvvvvv
    # åœ¨åˆ›å»ºPerformanceLoggeræ—¶ï¼Œç›´æ¥æŠŠå‡çš„daoâ€œæ³¨å…¥â€è¿›å»ï¼
    # è¿™æ ·å®ƒå°±ä¸ä¼šå†å»åˆ›å»ºçœŸå®çš„ã€éœ€è¦è¿æ¥æ•°æ®åº“çš„DAOäº†ã€‚
    performance_logger = PerformanceLogger(dao=mock_dao)
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    # ... (æµ‹è¯•è„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å®Œå…¨ä¿æŒä¸å˜) ...

    # å‡†å¤‡ä¸€ç‚¹å†å²æ•°æ®
    history_data = [
        LotteryHistory(period_number=str(2025001 + i),
                       front_area=random.sample(range(1, 36), 5),
                       back_area=random.sample(range(1, 13), 2))
        for i in range(50)
    ]
    base_algorithms = [FrequencyAnalysisAlgorithm(), BayesianNumberPredictor()]

    # ==========================================================
    # --- 2. æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡é¢„æµ‹ (å†·å¯åŠ¨) ---
    print("\n--- LOOP 1: Cold Start Prediction ---")
    weights_loop1 = mock_dao.get_average_scores_last_n_issues()
    optimizer_loop1 = DynamicEnsembleOptimizer(base_algorithms)
    optimizer_loop1.current_weights = weights_loop1
    optimizer_loop1.train(history_data)
    report_loop1 = optimizer_loop1.predict(history_data)
    print(f"\n[LOOP 1] Optimizer used weights: {optimizer_loop1.current_weights}")
    print(f"[LOOP 1] Final recommendation generated (front): {report_loop1['recommendations'][0]['front_numbers']}")

    # ==========================================================
    # --- 3. æ¨¡æ‹Ÿå¼€å¥–ï¼Œå¹¶è¿›è¡Œå­¦ä¹  ---
    print("\n--- POST-LOOP 1: Learning from Results ---")
    actual_draw_loop1 = LotteryHistory(period_number="2025051", front_area=[1, 2, 3, 4, 5], back_area=[1, 2])
    mock_model_outputs_loop1 = {
        'frequency_analysis': {'recommendations': [{'front_numbers': [3, 4, 5, 6, 7]}]},
        'bayesian_number_predictor': {'recommendations': [{'front_numbers': [10, 11, 12, 13, 14]}]}
    }
    performance_logger.evaluate_and_update(
        issue="2025051",
        model_outputs=mock_model_outputs_loop1,
        actual_draw=actual_draw_loop1
    )

    # ==========================================================
    # --- 4. æ¨¡æ‹Ÿç¬¬äºŒæ¬¡é¢„æµ‹ (çƒ­å¯åŠ¨) ---
    print("\n--- LOOP 2: Hot Start Prediction (with learned weights) ---")
    weights_loop2 = mock_dao.get_average_scores_last_n_issues()
    optimizer_loop2 = DynamicEnsembleOptimizer(base_algorithms)
    optimizer_loop2.current_weights = weights_loop2
    optimizer_loop2.train(history_data)
    report_loop2 = optimizer_loop2.predict(history_data)
    print(f"\n[LOOP 2] Optimizer started with LEARNED weights: {weights_loop2}")
    print(f"[LOOP 2] Final recommendation generated (front): {report_loop2['recommendations'][0]['front_numbers']}")

    print("\n" + "=" * 60)
    print("ğŸ  UNIT TEST COMPLETE")
    print("=" * 60)


if __name__ == "__main__":
    run_unit_test()