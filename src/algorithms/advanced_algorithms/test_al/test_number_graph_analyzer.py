# test_number_graph_analyzer.py
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# å·ç å›¾åˆ†æå™¨æµ‹è¯•è„šæœ¬
from src.database.database_manager import DatabaseManager
from src.config.database_config import DB_CONFIG

# å°è¯•å¯¼å…¥å·ç å›¾åˆ†æå™¨
try:
    from src.algorithms.advanced_algorithms.number_graph_analyzer import NumberGraphAnalyzer

    GRAPH_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥å·ç å›¾åˆ†æå™¨: {e}")
    GRAPH_AVAILABLE = False


def test_number_graph_analyzer():
    """æµ‹è¯•å·ç å›¾åˆ†æå™¨"""
    print("=== æµ‹è¯•å·ç å›¾åˆ†æå™¨ ===")

    if not GRAPH_AVAILABLE:
        print("âŒ å·ç å›¾åˆ†æå™¨ä¸å¯ç”¨ï¼Œè¯·å…ˆå®ç°è¯¥ç®—æ³•")
        create_graph_analysis_demo()
        return

    # åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
    predictor = NumberGraphAnalyzer()
    print(f"ç®—æ³•åç§°: {predictor.name}")
    print(f"ç‰ˆæœ¬: {predictor.version}")

    try:
        # ä»æ•°æ®åº“è·å–çœŸå®å†å²æ•°æ®
        print("\n1. è¿æ¥æ•°æ®åº“å¹¶è·å–å†å²æ•°æ®...")
        db_manager = DatabaseManager(**DB_CONFIG)

        # è·å–å†å²æ•°æ® - å›¾åˆ†æéœ€è¦è¶³å¤Ÿçš„æ•°æ®æ¥å»ºç«‹å…³ç³»
        history_data = db_manager.get_all_lottery_history(limit=200)
        print(f"ä»æ•°æ®åº“è·å–åˆ° {len(history_data)} æ¡å†å²è®°å½•")

        if not history_data:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰å†å²æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return

        # æ˜¾ç¤ºæ•°æ®èŒƒå›´
        if len(history_data) > 0:
            first_period = history_data[0].period_number
            last_period = history_data[-1].period_number
            print(f"æ•°æ®èŒƒå›´: ç¬¬{first_period}æœŸ - ç¬¬{last_period}æœŸ")

        # æµ‹è¯•è®­ç»ƒ
        print("\n2. æ„å»ºå·ç å…³ç³»å›¾...")
        train_success = predictor.train(history_data)
        print(f"è®­ç»ƒçŠ¶æ€: {'âœ… æˆåŠŸ' if train_success else 'âŒ å¤±è´¥'}")
        print(f"æ¨¡å‹å·²è®­ç»ƒ: {predictor.is_trained}")

        if not train_success:
            print("âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return

        # æµ‹è¯•é¢„æµ‹
        print("\n3. è¿›è¡Œå›¾åˆ†æé¢„æµ‹...")
        result = predictor.predict(history_data)

        if 'error' in result:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}")
            return

        # æ£€æŸ¥ç»“æœç»“æ„
        print("\n4. æ£€æŸ¥é¢„æµ‹ç»“æœ:")
        print(f"ç®—æ³•åç§°: {result.get('algorithm')}")
        print(f"ç‰ˆæœ¬: {result.get('version')}")
        print(f"ç½®ä¿¡åº¦: {result.get('recommendations', [{}])[0].get('confidence', 'N/A')}")

        # æ˜¾ç¤ºå‰åŒºå·ç è¯„åˆ†
        recommendations = result.get('recommendations', [{}])[0]
        front_scores = recommendations.get('front_number_scores', [])
        back_scores = recommendations.get('back_number_scores', [])

        print(f"\n5. å‰åŒºå·ç å›¾åˆ†æè¯„åˆ† (å‰15ä¸ª):")
        for i, score_item in enumerate(front_scores[:15]):
            print(f"  å·ç  {score_item['number']:2d}: è¯„åˆ† {score_item['score']:.4f}")

        print(f"\n6. ååŒºå·ç å›¾åˆ†æè¯„åˆ† (å‰8ä¸ª):")
        for i, score_item in enumerate(back_scores[:8]):
            print(f"  å·ç  {score_item['number']:2d}: è¯„åˆ† {score_item['score']:.4f}")

        # åŸºæœ¬éªŒè¯
        print(f"\n7. åŸºæœ¬éªŒè¯:")
        print(f"å‰åŒºå·ç æ•°é‡: {len(front_scores)} (åº”ä¸º35)")
        print(f"ååŒºå·ç æ•°é‡: {len(back_scores)} (åº”ä¸º12)")
        print(f"è¯„åˆ†èŒƒå›´æ­£å¸¸: {all(0 <= item['score'] <= 1 for item in front_scores + back_scores)}")

        # åˆ†æå›¾å…³ç³»ç‰¹å¾
        print(f"\n8. å›¾å…³ç³»ç‰¹å¾åˆ†æ:")
        front_top5 = front_scores[:5]
        back_top3 = back_scores[:3]

        print("   å‰åŒºä¸­å¿ƒæ€§æœ€é«˜å·ç :")
        for item in front_top5:
            print(f"     å·ç  {item['number']}: å›¾ä¸­å¿ƒæ€§è¯„åˆ† {item['score']:.4f}")

        print("   ååŒºä¸­å¿ƒæ€§æœ€é«˜å·ç :")
        for item in back_top3:
            print(f"     å·ç  {item['number']}: å›¾ä¸­å¿ƒæ€§è¯„åˆ† {item['score']:.4f}")

        # å¦‚æœæœ‰å›¾åˆ†æç›¸å…³ä¿¡æ¯ï¼Œæ˜¾ç¤ºå‡ºæ¥
        analysis = result.get('analysis', {})
        if 'graph_analysis' in analysis:
            graph_info = analysis['graph_analysis']
            print(f"\n9. å·ç å›¾åˆ†æä¿¡æ¯:")
            print(f"   å›¾èŠ‚ç‚¹æ•°: {graph_info.get('node_count', 'N/A')}")
            print(f"   å›¾è¾¹æ•°: {graph_info.get('edge_count', 'N/A')}")
            print(f"   å›¾å¯†åº¦: {graph_info.get('graph_density', 'N/A')}")

            # æ˜¾ç¤ºå¼ºå…³è”å·ç å¯¹
            if 'strong_connections' in graph_info:
                print(f"   æœ€å¼ºå…³è”å·ç å¯¹:")
                connections = graph_info['strong_connections'][:5]  # æ˜¾ç¤ºå‰5ä¸ª
                for conn in connections:
                    print(f"     {conn[0]} â†” {conn[1]}: {conn[2]:.4f}")

        print("\n=== å·ç å›¾åˆ†æå™¨æµ‹è¯•å®Œæˆ ===")

        return result

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


def create_graph_analysis_demo():
    """åˆ›å»ºå·ç å›¾åˆ†ææ¼”ç¤º"""
    print("\nğŸ•¸ï¸ åˆ›å»ºå·ç å›¾åˆ†ææ¼”ç¤º...")

    try:
        from src.database.database_manager import DatabaseManager
        import numpy as np
        from collections import defaultdict

        db_manager = DatabaseManager(**DB_CONFIG)
        history_data = db_manager.get_all_lottery_history(limit=100)

        if len(history_data) < 20:
            print("âŒ æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå›¾åˆ†æ")
            return

        print(f"ä½¿ç”¨ {len(history_data)} æ¡è®°å½•è¿›è¡Œå·ç å…³ç³»å›¾åˆ†æ")

        # æ„å»ºå…±ç°çŸ©é˜µ
        front_cooccurrence = build_cooccurrence_matrix(history_data, 'front')
        back_cooccurrence = build_cooccurrence_matrix(history_data, 'back')

        # åˆ†æå¼ºå…³è”
        front_strong_pairs = find_strong_connections(front_cooccurrence, 'front')
        back_strong_pairs = find_strong_connections(back_cooccurrence, 'back')

        print("âœ… å·ç å…³ç³»å›¾åˆ†æå®Œæˆ")
        print(f"   å‰åŒºå…³ç³»å¯¹: {len(front_strong_pairs)} ä¸ªå¼ºå…³è”")
        print(f"   ååŒºå…³ç³»å¯¹: {len(back_strong_pairs)} ä¸ªå¼ºå…³è”")

        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ”— å‰åŒºæœ€å¼ºå…³è”å·ç å¯¹:")
        for pair in front_strong_pairs[:10]:
            print(f"   {pair[0]} â†” {pair[1]}: å…±ç°{pair[2]}æ¬¡")

        print("\nğŸ”— ååŒºæœ€å¼ºå…³è”å·ç å¯¹:")
        for pair in back_strong_pairs[:5]:
            print(f"   {pair[0]} â†” {pair[1]}: å…±ç°{pair[2]}æ¬¡")

        # ç”ŸæˆåŸºäºå›¾ä¸­å¿ƒæ€§çš„é¢„æµ‹
        simulate_graph_prediction(history_data, front_cooccurrence, back_cooccurrence)

    except Exception as e:
        print(f"âŒ å·ç å›¾åˆ†ææ¼”ç¤ºå¤±è´¥: {e}")


def build_cooccurrence_matrix(history_data, area_type):
    """æ„å»ºå·ç å…±ç°çŸ©é˜µ"""
    size = 35 if area_type == 'front' else 12
    cooccurrence = np.zeros((size, size))

    for record in history_data:
        numbers = record.front_area if area_type == 'front' else record.back_area

        # æ›´æ–°å…±ç°è®¡æ•°
        for i in range(len(numbers)):
            for j in range(i + 1, len(numbers)):
                num1 = numbers[i] - 1
                num2 = numbers[j] - 1
                if 0 <= num1 < size and 0 <= num2 < size:
                    cooccurrence[num1][num2] += 1
                    cooccurrence[num2][num1] += 1

    return cooccurrence


def find_strong_connections(cooccurrence_matrix, area_type):
    """æ‰¾å‡ºå¼ºå…³è”å·ç å¯¹"""
    size = cooccurrence_matrix.shape[0]
    connections = []

    # è®¡ç®—å¹³å‡å…±ç°æ¬¡æ•°
    avg_cooccurrence = np.mean(cooccurrence_matrix)

    for i in range(size):
        for j in range(i + 1, size):
            if cooccurrence_matrix[i][j] > avg_cooccurrence * 1.5:
                connections.append((i + 1, j + 1, cooccurrence_matrix[i][j]))

    # æŒ‰å…±ç°æ¬¡æ•°æ’åº
    connections.sort(key=lambda x: x[2], reverse=True)
    return connections


def simulate_graph_prediction(history_data, front_cooccurrence, back_cooccurrence):
    """æ¨¡æ‹ŸåŸºäºå›¾åˆ†æçš„é¢„æµ‹"""
    print("\nğŸ¯ æ¨¡æ‹Ÿå›¾åˆ†æé¢„æµ‹ç»“æœ:")

    # è®¡ç®—åº¦æ•°ä¸­å¿ƒæ€§
    front_degree_centrality = calculate_degree_centrality(front_cooccurrence)
    back_degree_centrality = calculate_degree_centrality(back_cooccurrence)

    # å½’ä¸€åŒ–
    front_max = max(front_degree_centrality.values()) if front_degree_centrality else 1
    back_max = max(back_degree_centrality.values()) if back_degree_centrality else 1

    front_scores = [{'number': num, 'score': score / front_max}
                    for num, score in front_degree_centrality.items()]
    back_scores = [{'number': num, 'score': score / back_max}
                   for num, score in back_degree_centrality.items()]

    front_scores.sort(key=lambda x: x['score'], reverse=True)
    back_scores.sort(key=lambda x: x['score'], reverse=True)

    print("å‰åŒºå›¾ä¸­å¿ƒæ€§é¢„æµ‹ (å‰10ä¸ª):")
    for item in front_scores[:10]:
        print(f"  å·ç  {item['number']:2d}: ä¸­å¿ƒæ€§ {item['score']:.4f}")

    print("ååŒºå›¾ä¸­å¿ƒæ€§é¢„æµ‹ (å‰5ä¸ª):")
    for item in back_scores[:5]:
        print(f"  å·ç  {item['number']:2d}: ä¸­å¿ƒæ€§ {item['score']:.4f}")


def calculate_degree_centrality(cooccurrence_matrix):
    """è®¡ç®—åº¦æ•°ä¸­å¿ƒæ€§"""
    centrality = {}
    size = cooccurrence_matrix.shape[0]

    for i in range(size):
        # åº¦æ•°ä¸­å¿ƒæ€§ = ä¸è¯¥èŠ‚ç‚¹ç›¸è¿çš„è¾¹çš„æƒé‡å’Œ
        centrality[i + 1] = np.sum(cooccurrence_matrix[i])

    return centrality


if __name__ == "__main__":
    test_number_graph_analyzer()