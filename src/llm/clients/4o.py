import json
import requests
import pandas as pd
from datetime import datetime
import sys
import os

# æ·»åŠ é…ç½®æ–‡ä»¶è·¯å¾„
sys.path.append('E:/pyhton/AI/AICp/src/llm')
from src.llm.config import MODEL_CONFIG


class DltAdvancedAnalyzer:
    def __init__(self):
        """
        åˆå§‹åŒ–å¤§ä¹é€é«˜çº§åˆ†æå™¨
        """
        self.model_config = MODEL_CONFIG.get("gpt-4o", {})
        self.api_key = self.model_config.get("api_key")
        self.base_url = self.model_config.get("base_url")
        self.model = "gpt-4o"

        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        # åŠ è½½å†å²æ•°æ®
        self.history_data = self.load_history_data()

        print(f"âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(self.history_data)} æœŸå†å²æ•°æ®")

    def load_history_data(self):
        """
        åŠ è½½å¤§ä¹é€å†å²æ•°æ®
        """
        try:
            with open('E:/pyhton/AI/AICp/src/analysis/dlt_history_data.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
            return []

    def get_next_period_info(self):
        """
        è·å–ä¸‹ä¸€æœŸé¢„æµ‹ä¿¡æ¯
        """
        if not self.history_data:
            return None, None

        # è·å–æœ€è¿‘ä¸€æœŸæ•°æ®
        latest_data = self.history_data[-1]
        latest_period = latest_data['expect']
        latest_time = latest_data['time']

        # è®¡ç®—ä¸‹ä¸€æœŸï¼ˆå‡è®¾æœŸå·æ˜¯è¿ç»­çš„ï¼‰
        try:
            next_period = str(int(latest_period) + 1)
            # å‡è®¾å¼€å¥–æ—¶é—´é—´éš”ä¸º2-3å¤©
            latest_date = datetime.strptime(latest_time.split()[0], "%Y-%m-%d")
            next_date = latest_date.replace(day=latest_date.day + 2)
            next_time = next_date.strftime("%Y-%m-%d") + " 21:25:00"
        except:
            next_period = "ä¸‹ä¸€æœŸ"
            next_time = "è¿‘æœŸ"

        return next_period, next_time

    def prepare_recent_data(self, num_periods=100):
        """
        å‡†å¤‡æœ€è¿‘NæœŸçš„æ•°æ®ç”¨äºåˆ†æ
        """
        if not self.history_data:
            return "æ— å†å²æ•°æ®"

        recent_data = self.history_data[-num_periods:]
        formatted_data = []

        for item in recent_data:
            formatted_data.append({
                "æœŸå·": item['expect'],
                "å¼€å¥–æ—¶é—´": item['time'],
                "å‰åŒºå·ç ": item['frontArea'],
                "ååŒºå·ç ": item['backArea'],
                "å’Œå€¼": item['frontArea_Sum'],
                "å¥‡å¶æ¯”": item['frontArea_OddEven'],
                "æ˜¯å¦æœ‰è¿å·": item['frontArea_IsConsecutive'],
                "è·¨åº¦": item['frontArea_Span']
            })

        return formatted_data

    def prepare_simple_data_summary(self):
        """
        å‡†å¤‡ç®€å•çš„æ•°æ®æ‘˜è¦ï¼ˆä¸éœ€è¦å¤æ‚è®¡ç®—ï¼‰
        """
        if not self.history_data:
            return None

        print("ğŸ“Š å‡†å¤‡æ•°æ®æ‘˜è¦...")

        recent_data = self.history_data[-100:]  # ä½¿ç”¨æœ€è¿‘100æœŸ

        # ç®€å•ç»Ÿè®¡
        front_counts = {}
        back_counts = {}
        sums = []

        for item in recent_data:
            for num in item['frontArea']:
                front_counts[num] = front_counts.get(num, 0) + 1
            for num in item['backArea']:
                back_counts[num] = back_counts.get(num, 0) + 1
            sums.append(item['frontArea_Sum'])

        # çƒ­é—¨å·ç 
        front_hot = sorted(front_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        back_hot = sorted(back_counts.items(), key=lambda x: x[1], reverse=True)[:5]

        data_summary = {
            "periods_analyzed": len(recent_data),
            "date_range": f"{recent_data[0]['time']} è‡³ {recent_data[-1]['time']}",
            "front_hot_numbers": [x[0] for x in front_hot],
            "back_hot_numbers": [x[0] for x in back_hot],
            "average_sum": sum(sums) / len(sums),
            "recent_20_data": recent_data[-20:]  # æœ€è¿‘20æœŸè¯¦ç»†æ•°æ®
        }

        return data_summary

    def call_gpt4o_advanced_analysis(self, prompt: str) -> dict:
        """
        è°ƒç”¨GPT-4oè¿›è¡Œé«˜çº§åˆ†æ
        """
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ªé¡¶å°–çš„å½©ç¥¨é¢„æµ‹ä¸“å®¶å’Œæ•°æ®åˆ†æç§‘å­¦å®¶ã€‚è¯·åŸºäºæä¾›çš„å†å²æ•°æ®ï¼Œåœ¨è„‘æµ·ä¸­æ¨¡æ‹Ÿè¿è¡Œå„ç§é«˜çº§ç®—æ³•è¿›è¡Œåˆ†æã€‚

åˆ†æè¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨æ€ç»´ä¸­æ¨¡æ‹Ÿè¿è¡Œå› æœæ¨æ–­ã€æ—¶é—´åºåˆ—é¢„æµ‹ç­‰é«˜çº§ç®—æ³•
2. æ¯ä¸ªé¢„æµ‹å¿…é¡»æœ‰æ•°å­¦å’Œç»Ÿè®¡ç†è®ºæ”¯æŒ
3. æä¾›è¯¦ç»†çš„ç®—æ³•æ¨¡æ‹Ÿè¿‡ç¨‹å’Œæ¨ç†
4. åŸºäºçœŸå®æ¦‚ç‡è®¡ç®—è€Œéä¸»è§‚çŒœæµ‹
5. ç»™å‡ºå¤šä¸ªç®—æ³•è§†è§’çš„äº¤å‰éªŒè¯
6. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœ"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.2,  # è¾ƒä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
            "max_tokens": 16000,  # å¢åŠ tokené™åˆ¶ä»¥å®¹çº³è¯¦ç»†åˆ†æ
            "response_format": {"type": "json_object"}
        }

        try:
            print("ğŸ”„ æ­¥éª¤1: å¼€å§‹è°ƒç”¨GPT-4oè¿›è¡Œæ·±åº¦åˆ†æ...")
            print(f"   ğŸ“¡ è¯·æ±‚åœ°å€: {self.base_url}")
            print(f"   ğŸ”‘ ä½¿ç”¨æ¨¡å‹: {self.model}")
            print(f"   ğŸ“Š æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

            start_time = datetime.now()
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=180  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            end_time = datetime.now()
            request_duration = (end_time - start_time).total_seconds()

            print(f"   â±ï¸  APIè¯·æ±‚è€—æ—¶: {request_duration:.2f}ç§’")
            print(f"   ğŸ“¨ å“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                print("   âœ… APIè°ƒç”¨æˆåŠŸï¼Œå¼€å§‹è§£æè¿”å›å†…å®¹...")

                # è§£æJSONå†…å®¹
                parsed_result = json.loads(content)
                print(f"   ğŸ“‹ è¿”å›æ•°æ®ç»“æ„: {list(parsed_result.keys())}")

                return parsed_result
            else:
                print(f"   âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                print(f"   ğŸ” é”™è¯¯è¯¦æƒ…: {response.text[:500]}...")
                return {"error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code}", "details": response.text}

        except json.JSONDecodeError as e:
            print(f"   âŒ JSONè§£æé”™è¯¯: {e}")
            if 'content' in locals():
                print(f"   ğŸ“„ åŸå§‹è¿”å›å†…å®¹: {content[:500]}...")
            return {"error": "JSONè§£æå¤±è´¥", "details": str(e)}
        except Exception as e:
            print(f"   âŒ è°ƒç”¨è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            print(f"   ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {"error": "è¯·æ±‚å¼‚å¸¸", "details": str(e)}

    def generate_detailed_analysis_prompt(self, data_summary: dict, next_period: str, next_time: str) -> str:
        """
        ç”Ÿæˆè¯¦ç»†åˆ†ææç¤ºè¯ï¼ŒåŒ…å«ç®—æ³•è¿‡ç¨‹å’Œå¤šç§ç»„åˆ
        """
        prompt = f"""
# å¤§ä¹é€é«˜çº§ç®—æ³•é¢„æµ‹åˆ†æ
## é¢„æµ‹ç›®æ ‡ï¼šç¬¬{next_period}æœŸå¤§ä¹é€

## å¯ç”¨å†å²æ•°æ®
- åˆ†ææœŸæ•°ï¼š{data_summary['periods_analyzed']}æœŸ
- æ—¶é—´èŒƒå›´ï¼š{data_summary['date_range']}
- å‰åŒºçƒ­é—¨å·ç ï¼š{data_summary['front_hot_numbers']}
- ååŒºçƒ­é—¨å·ç ï¼š{data_summary['back_hot_numbers']}
- å¹³å‡å’Œå€¼ï¼š{data_summary['average_sum']:.1f}

## è¯¦ç»†å†å²æ•°æ®ï¼ˆæœ€è¿‘20æœŸï¼‰
{json.dumps(data_summary['recent_20_data'], ensure_ascii=False, indent=2)}

## ç®—æ³•æ¡†æ¶è¦æ±‚

è¯·åŸºäºä¸Šè¿°å†å²æ•°æ®ï¼Œåœ¨æ€ç»´ä¸­æ¨¡æ‹Ÿè¿è¡Œä»¥ä¸‹é«˜çº§ç®—æ³•ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„åˆ†æè¿‡ç¨‹ï¼š

### ç®—æ³•1ï¼šè´å¶æ–¯ç»“æ„æ—¶é—´åºåˆ—ï¼ˆBSTSï¼‰æ¨¡æ‹Ÿ
**ç®—æ³•åŸç†ï¼š**
çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼šy_t = Z_t * Î±_t + Îµ_t, Î±_t = T_t * Î±_{{t-1}} + R_t * Î·_t
ä½¿ç”¨MCMCè¿›è¡ŒåéªŒé‡‡æ ·ï¼Œè·å¾—é¢„æµ‹åˆ†å¸ƒ

**æ¨¡æ‹Ÿåˆ†ææ­¥éª¤ï¼š**
1. æ„å»ºçŠ¶æ€ç©ºé—´ç»„ä»¶ï¼ˆå±€éƒ¨çº¿æ€§è¶‹åŠ¿ã€å­£èŠ‚æ€§ã€å›å½’é¡¹ï¼‰
2. è¿è¡ŒMCMCé‡‡æ ·ï¼ˆ10000æ¬¡è¿­ä»£ï¼Œburn-in 1000æ¬¡ï¼‰
3. è®¡ç®—åéªŒé¢„æµ‹åˆ†å¸ƒ p(y_{{t+1}}|y_{{1:t}})
4. æå–å·ç å‡ºç°çš„åéªŒæ¦‚ç‡

**è¾“å‡ºè¦æ±‚ï¼š**
- åéªŒæ¦‚ç‡åˆ†å¸ƒçŸ©é˜µ
- MCMCæ”¶æ•›è¯Šæ–­
- é¢„æµ‹åŒºé—´ä¼°è®¡

### ç®—æ³•2ï¼šå› æœæ£®æ— + åŒé‡æœºå™¨å­¦ä¹ æ¨¡æ‹Ÿ
**ç®—æ³•åŸç†ï¼š**
å¤„ç†æ•ˆåº”ä¼°è®¡ï¼šÏ„(x) = E[Y(1)-Y(0)|X=x]
ä½¿ç”¨å› æœæ£®æ—è¿›è¡Œå¼‚è´¨æ€§å¤„ç†æ•ˆåº”ä¼°è®¡
åŒé‡æœºå™¨å­¦ä¹ å»åï¼šÎ¸Ì‚ = argmin_Î¸ E[Ïˆ(W;Î¸,Î·â‚€)]

**æ¨¡æ‹Ÿåˆ†ææ­¥éª¤ï¼š**
1. æ„å»ºç‰¹å¾çŸ©é˜µXï¼ˆåŒ…å«å†å²å‡ºç°é¢‘ç‡ã€é—æ¼å€¼ç­‰ï¼‰
2. ä½¿ç”¨å› æœæ£®æ—ä¼°è®¡æ¯ä¸ªå·ç çš„Conditional Average Treatment Effect (CATE)
3. åº”ç”¨åŒé‡æœºå™¨å­¦ä¹ è¿›è¡Œåå·®æ ¡æ­£
4. è®¡ç®—ç¨³å¥æ ‡å‡†è¯¯å’Œç½®ä¿¡åŒºé—´

**è¾“å‡ºè¦æ±‚ï¼š**
- æ¯ä¸ªå·ç çš„CATEä¼°è®¡å€¼
- å› æœæ•ˆåº”æ˜¾è‘—æ€§æ£€éªŒ
- å¼‚è´¨æ€§åˆ†æç»“æœ

### ç®—æ³•3ï¼šLSTM-Transformeræ··åˆæ¨¡å‹æ¨¡æ‹Ÿ
**ç®—æ³•åŸç†ï¼š**
LSTMï¼šh_t = LSTM(x_t, h_{{t-1}})
Transformerï¼šAttention(Q,K,V) = softmax(QK^T/âˆšd_k)V
æ··åˆæ¶æ„ç»“åˆæ—¶åºå»ºæ¨¡å’Œæ³¨æ„åŠ›æœºåˆ¶

**æ¨¡æ‹Ÿåˆ†ææ­¥éª¤ï¼š**
1. æ„å»ºåºåˆ—æ•°æ®ï¼ˆæ»‘åŠ¨çª—å£=10æœŸï¼‰
2. LSTMå±‚æ•æ‰é•¿æœŸä¾èµ–å…³ç³»
3. Transformerå¤šå¤´æ³¨æ„åŠ›æ•æ‰å·ç é—´å…³è”
4. è¾“å‡ºå±‚ä½¿ç”¨softmaxè·å¾—æ¦‚ç‡åˆ†å¸ƒ
5. ä½¿ç”¨äº¤å‰ç†µæŸå¤±å’ŒAdamä¼˜åŒ–å™¨

**è¾“å‡ºè¦æ±‚ï¼š**
- åºåˆ—é¢„æµ‹æ¦‚ç‡
- æ³¨æ„åŠ›æƒé‡åˆ†æ
- è®­ç»ƒéªŒè¯æ›²çº¿

### ç®—æ³•4ï¼šé«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆGPRï¼‰æ¨¡æ‹Ÿ
**ç®—æ³•åŸç†ï¼š**
é«˜æ–¯è¿‡ç¨‹ï¼šf(x) ~ GP(m(x), k(x,x'))
ä½¿ç”¨Maternæ ¸å‡½æ•°ï¼šk_{{Î½=3/2}}(r) = (1 + âˆš3r/â„“)exp(-âˆš3r/â„“)
è´å¶æ–¯éå‚æ•°å»ºæ¨¡

**æ¨¡æ‹Ÿåˆ†ææ­¥éª¤ï¼š**
1. é€‰æ‹©Matern 3/2æ ¸å‡½æ•°
2. ä¼˜åŒ–è¶…å‚æ•°ï¼ˆé•¿åº¦å°ºåº¦â„“ï¼Œæ–¹å·®ÏƒÂ²ï¼‰
3. è®¡ç®—åéªŒé¢„æµ‹åˆ†å¸ƒ
4. è·å¾—é¢„æµ‹å‡å€¼å’Œæ–¹å·®

**è¾“å‡ºè¦æ±‚ï¼š**
- é¢„æµ‹åˆ†å¸ƒå‚æ•°
- ä¸ç¡®å®šæ€§é‡åŒ–
- æ ¸å‡½æ•°è¶…å‚æ•°

### ç®—æ³•5ï¼šé›†æˆå› æœæ¨æ–­æ¨¡æ‹Ÿ
**ç®—æ³•åŸç†ï¼š**
å…ƒå­¦ä¹ å™¨é›†æˆï¼šÏ„Ì‚(x) = âˆ‘_{{m=1}}^M w_m Ï„Ì‚_m(x)
ç»“åˆå¤šç§å› æœæ¨æ–­æ–¹æ³•ï¼ˆå€¾å‘å¾—åˆ†åŒ¹é…ã€å·¥å…·å˜é‡ã€å›å½’æ–­ç‚¹è®¾è®¡ï¼‰

**æ¨¡æ‹Ÿåˆ†ææ­¥éª¤ï¼š**
1. è¿è¡Œå¤šä¸ªåŸºç¡€å› æœæ¨¡å‹
2. è®¡ç®—æ¨¡å‹æƒé‡ï¼ˆåŸºäºæ ·æœ¬å¤–è¡¨ç°ï¼‰
3. å…ƒå­¦ä¹ å™¨é›†æˆé¢„æµ‹
4. å› æœä¸€è‡´æ€§æ£€éªŒ

**è¾“å‡ºè¦æ±‚ï¼š**
- é›†æˆæƒé‡åˆ†å¸ƒ
- å› æœå‘ç°ç»“æœ
- ç¨³å¥æ€§æ£€éªŒ

## æ¦‚ç‡è®¡ç®—æ¡†æ¶

### è”åˆæ¦‚ç‡åˆ†å¸ƒ
P(å·ç ç»„åˆ) = âˆ_{{i=1}}^5 P(å‰åŒº_i) Ã— âˆ_{{j=1}}^2 P(ååŒº_j)

### æ¡ä»¶æ¦‚ç‡è®¡ç®—
ä½¿ç”¨è´å¶æ–¯å®šç†ï¼šP(A|B) = P(B|A)P(A)/P(B)

### æœŸæœ›å€¼è®¡ç®—
E[æ”¶ç›Š] = âˆ‘ ä¸­å¥–æ¦‚ç‡ Ã— ä¸­å¥–é‡‘é¢ - æˆæœ¬

## è¾“å‡ºæ ¼å¼è¦æ±‚

{{
    "prediction_metadata": {{
        "target_period": "{next_period}",
        "algorithms_simulated": [
            "è´å¶æ–¯ç»“æ„æ—¶é—´åºåˆ—(BSTS)",
            "å› æœæ£®æ—+åŒé‡æœºå™¨å­¦ä¹ ",
            "LSTM-Transformeræ··åˆæ¨¡å‹",
            "é«˜æ–¯è¿‡ç¨‹å›å½’(GPR)",
            "é›†æˆå› æœæ¨æ–­"
        ],
        "mathematical_framework": "ä½¿ç”¨çš„æ•°å­¦æ¡†æ¶æè¿°",
        "simulation_accuracy": "ç®—æ³•æ¨¡æ‹Ÿç²¾åº¦è¯„ä¼°"
    }},
    "algorithm_simulation_details": {{
        "bsts_simulation": {{
            "model_specification": "çŠ¶æ€ç©ºé—´æ¨¡å‹è®¾å®šè¯¦æƒ…",
            "mcmc_simulation": "MCMCæ¨¡æ‹Ÿè¿‡ç¨‹æè¿°",
            "posterior_predictive_distribution": "åéªŒé¢„æµ‹åˆ†å¸ƒç»“æœ",
            "convergence_diagnostics": "æ”¶æ•›è¯Šæ–­ç»“æœ",
            "number_probabilities": "åŸºäºBSTSçš„å·ç æ¦‚ç‡"
        }},
        "causal_forest_simulation": {{
            "feature_engineering": "ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹",
            "causal_effect_estimation": "å› æœæ•ˆåº”ä¼°è®¡ç»“æœ",
            "double_ml_correction": "åŒé‡æœºå™¨å­¦ä¹ æ ¡æ­£",
            "heterogeneity_analysis": "å¼‚è´¨æ€§åˆ†æ",
            "robustness_tests": "ç¨³å¥æ€§æ£€éªŒç»“æœ"
        }},
        "lstm_transformer_simulation": {{
            "architecture_design": "æ¨¡å‹æ¶æ„è®¾è®¡",
            "sequence_modeling": "åºåˆ—å»ºæ¨¡è¿‡ç¨‹",
            "attention_analysis": "æ³¨æ„åŠ›æœºåˆ¶åˆ†æ",
            "training_simulation": "è®­ç»ƒè¿‡ç¨‹æ¨¡æ‹Ÿ",
            "predictive_performance": "é¢„æµ‹æ€§èƒ½è¯„ä¼°"
        }},
        "gaussian_process_simulation": {{
            "kernel_selection": "æ ¸å‡½æ•°é€‰æ‹©ç†ç”±",
            "hyperparameter_optimization": "è¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹",
            "predictive_distribution": "é¢„æµ‹åˆ†å¸ƒç»“æœ",
            "uncertainty_quantification": "ä¸ç¡®å®šæ€§é‡åŒ–",
            "model_validation": "æ¨¡å‹éªŒè¯ç»“æœ"
        }},
        "ensemble_causal_simulation": {{
            "base_learners": "åŸºç¡€å­¦ä¹ å™¨é…ç½®",
            "meta_learning_process": "å…ƒå­¦ä¹ è¿‡ç¨‹",
            "causal_discovery": "å› æœå‘ç°ç»“æœ",
            "ensemble_weights": "é›†æˆæƒé‡è®¡ç®—",
            "consistency_checks": "ä¸€è‡´æ€§æ£€éªŒ"
        }}
    }},
    "probability_calculations": {{
        "joint_probability_analysis": "è”åˆæ¦‚ç‡åˆ†æ",
        "conditional_probability_calculation": "æ¡ä»¶æ¦‚ç‡è®¡ç®—",
        "bayesian_updating_process": "è´å¶æ–¯æ›´æ–°è¿‡ç¨‹",
        "expected_value_analysis": "æœŸæœ›å€¼åˆ†æ",
        "risk_adjusted_probabilities": "é£é™©è°ƒæ•´åæ¦‚ç‡"
    }},
    "cross_algorithm_validation": {{
        "algorithm_agreement_analysis": "ç®—æ³•ä¸€è‡´æ€§åˆ†æ",
        "prediction_concordance": "é¢„æµ‹ä¸€è‡´æ€§æ£€éªŒ",
        "confidence_interval_overlap": "ç½®ä¿¡åŒºé—´é‡å åˆ†æ",
        "robustness_assessment": "ç¨³å¥æ€§è¯„ä¼°"
    }},
    "optimized_recommendations": {{
        "7_3_combination_bsts_optimized": {{
            "algorithm_basis": "è´å¶æ–¯ç»“æ„æ—¶é—´åºåˆ—ä¼˜åŒ–",
            "front_numbers": [1, 2, 3, 4, 5, 6, 7],
            "back_numbers": [1, 2, 3],
            "mathematical_justification": {{
                "posterior_probability_calculation": "åéªŒæ¦‚ç‡è®¡ç®—è¿‡ç¨‹",
                "credible_intervals": "å¯ä¿¡åŒºé—´ä¼°è®¡",
                "model_evidence": "æ¨¡å‹è¯æ®å€¼",
                "predictive_performance": "é¢„æµ‹æ€§èƒ½æŒ‡æ ‡"
            }},
            "risk_assessment": {{
                "volatility_analysis": "æ³¢åŠ¨æ€§åˆ†æ",
                "downside_protection": "ä¸‹è¡Œä¿æŠ¤è¯„ä¼°",
                "value_at_risk": "é£é™©ä»·å€¼è®¡ç®—"
            }},
            "confidence_metrics": {{
                "calibrated_confidence": 0.92,
                "prediction_intervals": "é¢„æµ‹åŒºé—´",
                "sensitivity_analysis": "æ•æ„Ÿæ€§åˆ†æç»“æœ"
            }}
        }},
        "6_3_combination_causal_optimized": {{
            "algorithm_basis": "å› æœæ¨æ–­ä¼˜åŒ–",
            "front_numbers": [8, 9, 10, 11, 12, 13],
            "back_numbers": [4, 5, 6],
            "mathematical_justification": {{
                "causal_effect_sizes": "å› æœæ•ˆåº”å¤§å°",
                "instrumental_variable_analysis": "å·¥å…·å˜é‡åˆ†æ",
                "propensity_score_matching": "å€¾å‘å¾—åˆ†åŒ¹é…",
                "treatment_heterogeneity": "å¤„ç†å¼‚è´¨æ€§"
            }},
            "risk_assessment": {{
                "causal_risk_factors": "å› æœé£é™©å› ç´ ",
                "confounding_control": "æ··æ‚å› ç´ æ§åˆ¶",
                "selection_bias_assessment": "é€‰æ‹©åå€šè¯„ä¼°"
            }},
            "confidence_metrics": {{
                "calibrated_confidence": 0.89,
                "causal_identification_strength": "å› æœè¯†åˆ«å¼ºåº¦",
                "robustness_score": 0.87
            }}
        }},
        "7_2_combination_deep_learning_optimized": {{
            "algorithm_basis": "æ·±åº¦å­¦ä¹ ä¼˜åŒ–",
            "front_numbers": [14, 15, 16, 17, 18, 19, 20],
            "back_numbers": [7, 8],
            "mathematical_justification": {{
                "neural_network_architecture": "ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡",
                "attention_mechanism_analysis": "æ³¨æ„åŠ›æœºåˆ¶åˆ†æ",
                "gradient_flow_analysis": "æ¢¯åº¦æµåˆ†æ",
                "regularization_effects": "æ­£åˆ™åŒ–æ•ˆæœ"
            }},
            "risk_assessment": {{
                "overfitting_risk": "è¿‡æ‹Ÿåˆé£é™©",
                "generalization_error": "æ³›åŒ–è¯¯å·®",
                "model_uncertainty": "æ¨¡å‹ä¸ç¡®å®šæ€§"
            }},
            "confidence_metrics": {{
                "calibrated_confidence": 0.86,
                "cross_validation_scores": "äº¤å‰éªŒè¯åˆ†æ•°",
                "out_of_sample_performance": "æ ·æœ¬å¤–è¡¨ç°"
            }}
        }},
        "5_2_combination_gaussian_optimized": {{
            "algorithm_basis": "é«˜æ–¯è¿‡ç¨‹ä¼˜åŒ–",
            "front_numbers": [21, 22, 23, 24, 25],
            "back_numbers": [9, 10],
            "mathematical_justification": {{
                "gaussian_process_model": "é«˜æ–¯è¿‡ç¨‹æ¨¡å‹",
                "kernel_function_analysis": "æ ¸å‡½æ•°åˆ†æ",
                "marginal_likelihood_maximization": "è¾¹ç¼˜ä¼¼ç„¶æœ€å¤§åŒ–",
                "hyperparameter_posterior": "è¶…å‚æ•°åéªŒ"
            }},
            "risk_assessment": {{
                "predictive_variance_analysis": "é¢„æµ‹æ–¹å·®åˆ†æ",
                "uncertainty_propagation": "ä¸ç¡®å®šæ€§ä¼ æ’­",
                "model_misspecification_risk": "æ¨¡å‹è®¾å®šé”™è¯¯é£é™©"
            }},
            "confidence_metrics": {{
                "calibrated_confidence": 0.83,
                "predictive_interval_coverage": "é¢„æµ‹åŒºé—´è¦†ç›–",
                "model_evidence": "æ¨¡å‹è¯æ®"
            }}
        }},
        "5_2_combination_ensemble_optimized": {{
            "algorithm_basis": "é›†æˆå­¦ä¹ ä¼˜åŒ–",
            "front_numbers": [26, 27, 28, 29, 30],
            "back_numbers": [11, 12],
            "mathematical_justification": {{
                "ensemble_methodology": "é›†æˆæ–¹æ³•å­¦",
                "model_weighting_scheme": "æ¨¡å‹åŠ æƒæ–¹æ¡ˆ",
                "diversity_analysis": "å¤šæ ·æ€§åˆ†æ",
                "meta_learning_strategy": "å…ƒå­¦ä¹ ç­–ç•¥"
            }},
            "risk_assessment": {{
                "ensemble_risk_factors": "é›†æˆé£é™©å› ç´ ",
                "correlation_analysis": "ç›¸å…³æ€§åˆ†æ",
                "model_dependency_risk": "æ¨¡å‹ä¾èµ–æ€§é£é™©"
            }},
            "confidence_metrics": {{
                "calibrated_confidence": 0.80,
                "ensemble_stability": "é›†æˆç¨³å®šæ€§",
                "agreement_metrics": "ä¸€è‡´æ€§æŒ‡æ ‡"
            }}
        }}
    }},
    "final_recommendation_summary": {{
        "optimal_strategy_selection": "æœ€ä¼˜ç­–ç•¥é€‰æ‹©",
        "risk_diversification_plan": "é£é™©åˆ†æ•£è®¡åˆ’",
        "investment_allocation_advice": "èµ„é‡‘åˆ†é…å»ºè®®",
        "overall_confidence_assessment": {{
            "integrated_confidence_score": 0.88,
            "validation_metrics_summary": "éªŒè¯æŒ‡æ ‡æ€»ç»“",
            "robustness_evaluation": "ç¨³å¥æ€§è¯„ä¼°"
        }},
        "mathematical_certainty_level": "æ•°å­¦ç¡®å®šæ€§æ°´å¹³"
    }}
}}

## é‡è¦è¯´æ˜

è¯·åŸºäºæä¾›çš„å†å²æ•°æ®ï¼Œåœ¨æ€ç»´ä¸­è¯¦ç»†æ¨¡æ‹Ÿè¿è¡Œä¸Šè¿°æ‰€æœ‰é«˜çº§ç®—æ³•ã€‚æ¯ä¸ªç®—æ³•éƒ½è¦æä¾›ï¼š
1. å®Œæ•´çš„æ•°å­¦æ¨å¯¼è¿‡ç¨‹
2. å‚æ•°ä¼°è®¡å’Œä¼˜åŒ–æ­¥éª¤
3. é¢„æµ‹ç»“æœå’Œä¸ç¡®å®šæ€§é‡åŒ–
4. æ¨¡å‹éªŒè¯å’Œè¯Šæ–­

æ‰€æœ‰æ¨èå¿…é¡»åŸºäºä¸¥æ ¼çš„æ•°å­¦è®¡ç®—å’Œç»Ÿè®¡æ¨æ–­ï¼Œè€Œä¸æ˜¯ä¸»è§‚çŒœæµ‹ã€‚

ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç¬¬{next_period}æœŸçš„ä¸­å¥–æ¦‚ç‡ï¼ŒåŒæ—¶æ§åˆ¶é£é™©ã€‚
"""

        return prompt

    def analyze_dlt_comprehensive(self):
        """
        æ‰§è¡Œå¤§ä¹é€ç»¼åˆåˆ†æ
        """
        print("ğŸ¯ å¼€å§‹å¤§ä¹é€é«˜çº§åˆ†ææµç¨‹...")
        print("=" * 60)

        # è·å–ä¸‹ä¸€æœŸä¿¡æ¯
        print("ğŸ“… æ­¥éª¤1: ç¡®å®šé¢„æµ‹ç›®æ ‡æœŸå·...")
        next_period, next_time = self.get_next_period_info()
        if not next_period:
            print("âŒ é”™è¯¯: æ— æ³•ç¡®å®šä¸‹ä¸€æœŸä¿¡æ¯")
            return {"status": "error", "message": "æ— æ³•ç¡®å®šä¸‹ä¸€æœŸä¿¡æ¯"}

        print(f"   ğŸ¯ é¢„æµ‹ç›®æ ‡: ç¬¬{next_period}æœŸ ({next_time})")

        # å‡†å¤‡å†å²æ•°æ®
        print("ğŸ“‚ æ­¥éª¤2: åŠ è½½å†å²æ•°æ®...")
        data_summary = self.prepare_simple_data_summary()
        if not data_summary:
            print("âŒ é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„å†å²æ•°æ®")
            return {"status": "error", "message": "æ— å†å²æ•°æ®"}

        print(f"   âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{data_summary['periods_analyzed']}æœŸå†å²æ•°æ®")

        # ç”Ÿæˆè¯¦ç»†æç¤ºè¯
        print("\nğŸ“ æ­¥éª¤3: ç”Ÿæˆè¯¦ç»†åˆ†ææç¤ºè¯...")
        prompt = self.generate_detailed_analysis_prompt(data_summary, next_period, next_time)
        print(f"   âœ… æç¤ºè¯ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")

        # è°ƒç”¨APIè¿›è¡Œåˆ†æ
        print("\nğŸ¤– æ­¥éª¤4: è°ƒç”¨GPT-4oè¿›è¡Œæ·±åº¦ç®—æ³•åˆ†æ...")
        print("   æ­£åœ¨åº”ç”¨ä»¥ä¸‹ç®—æ³•:")
        print("   1. è´å¶æ–¯ç»“æ„æ—¶é—´åºåˆ— (BSTS) - çŠ¶æ€ç©ºé—´å»ºæ¨¡ + MCMC")
        print("   2. å› æœæ£®æ— + åŒé‡æœºå™¨å­¦ä¹  - å› æœæ¨æ–­ + å»å")
        print("   3. LSTM-Transformeræ··åˆæ¨¡å‹ - æ·±åº¦å­¦ä¹ åºåˆ—é¢„æµ‹")
        print("   4. é«˜æ–¯è¿‡ç¨‹å›å½’ (GPR) - è´å¶æ–¯éå‚æ•°å»ºæ¨¡")
        print("   5. é›†æˆå› æœæ¨æ–­ - å…ƒå­¦ä¹ å™¨é›†æˆ")

        result = self.call_gpt4o_advanced_analysis(prompt)

        if result and "error" not in result:
            print("\nğŸ“Š æ­¥éª¤5: å¤„ç†åˆ†æç»“æœ...")
            self.display_algorithm_results(result, next_period)
            save_result = self.save_algorithm_results(result)

            final_result = {
                "status": "success",
                "prediction_period": next_period,
                "analysis_result": result,
                "save_status": save_result,
                "timestamp": datetime.now().isoformat()
            }
            print("ğŸ‰ ç»¼åˆåˆ†ææµç¨‹å®Œæˆ!")
            return final_result
        else:
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯") if result else "åˆ†æå¤±è´¥"
            print(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {error_msg}")

            return {
                "status": "error",
                "message": error_msg,
                "prediction_period": next_period,
                "timestamp": datetime.now().isoformat(),
                "details": result.get("details") if result else "æœªçŸ¥é”™è¯¯"
            }

    def display_algorithm_results(self, result: dict, next_period: str):
        """
        æ˜¾ç¤ºç®—æ³•æ¨¡æ‹Ÿç»“æœ
        """
        print("\n" + "=" * 80)
        print("ğŸ¯ é«˜çº§ç®—æ³•æ¨¡æ‹Ÿé¢„æµ‹æŠ¥å‘Š")
        print("=" * 80)

        # å…ƒæ•°æ®
        metadata = result.get("prediction_metadata", {})
        print(f"\nğŸ“… é¢„æµ‹ä¿¡æ¯:")
        print(f"   ç›®æ ‡æœŸå·: {next_period}")
        algorithms = metadata.get("algorithms_simulated", [])
        if algorithms:
            print(f"   æ¨¡æ‹Ÿç®—æ³•: {', '.join(algorithms[:3])}...")
        math_framework = metadata.get('mathematical_framework', 'N/A')
        if math_framework and len(math_framework) > 50:
            print(f"   æ•°å­¦æ¡†æ¶: {math_framework[:50]}...")
        else:
            print(f"   æ•°å­¦æ¡†æ¶: {math_framework}")

        # ç®—æ³•æ¨¡æ‹Ÿè¯¦æƒ…
        algorithm_details = result.get("algorithm_simulation_details", {})
        print(f"\nğŸ”¬ ç®—æ³•æ¨¡æ‹Ÿæ‘˜è¦:")

        for algo_name, algo_detail in algorithm_details.items():
            if isinstance(algo_detail, dict):
                algo_name_pretty = algo_name.replace('_', ' ').title()
                first_key = next(iter(algo_detail.keys()), "")
                print(f"   {algo_name_pretty}: {first_key}")

        # æ¨èç»„åˆ
        recommendations = result.get("optimized_recommendations", {})
        print(f"\nğŸ’¡ ä¼˜åŒ–æ¨èç»„åˆ:")

        combo_mapping = {
            "7_3_combination_bsts_optimized": "7+3ç»„åˆ(BSTSä¼˜åŒ–)",
            "6_3_combination_causal_optimized": "6+3ç»„åˆ(å› æœä¼˜åŒ–)",
            "7_2_combination_deep_learning_optimized": "7+2ç»„åˆ(æ·±åº¦å­¦ä¹ ä¼˜åŒ–)",
            "5_2_combination_gaussian_optimized": "5+2ç»„åˆ1(é«˜æ–¯ä¼˜åŒ–)",
            "5_2_combination_ensemble_optimized": "5+2ç»„åˆ2(é›†æˆä¼˜åŒ–)"
        }

        for combo_key, combo_name in combo_mapping.items():
            combo = recommendations.get(combo_key, {})
            if combo:
                print(f"\n   {combo_name}:")
                print(f"      ç®—æ³•åŸºç¡€: {combo.get('algorithm_basis', 'N/A')}")
                print(f"      å‰åŒºå·ç : {combo.get('front_numbers', [])}")
                print(f"      ååŒºå·ç : {combo.get('back_numbers', [])}")

                confidence_metrics = combo.get("confidence_metrics", {})
                calibrated_conf = confidence_metrics.get("calibrated_confidence", 0)
                print(f"      æ ¡å‡†ç½®ä¿¡åº¦: {calibrated_conf * 100}%")

                # æ•°å­¦ä¾æ®
                math_just = combo.get("mathematical_justification", {})
                if isinstance(math_just, dict) and math_just:
                    first_key = next(iter(math_just.keys()), "")
                    print(f"      æ•°å­¦ä¾æ®: {first_key}")

        # æœ€ç»ˆæ¨èæ‘˜è¦
        final_summary = result.get("final_recommendation_summary", {})
        overall_conf = final_summary.get("overall_confidence_assessment", {})
        integrated_conf = overall_conf.get("integrated_confidence_score", 0)
        print(f"\nğŸ“Š æœ€ç»ˆæ¨èæ‘˜è¦:")
        print(f"   ç»¼åˆç½®ä¿¡åº¦: {integrated_conf * 100}%")

        optimal_strategy = final_summary.get('optimal_strategy_selection', 'N/A')
        if optimal_strategy and len(optimal_strategy) > 50:
            print(f"   æœ€ä¼˜ç­–ç•¥: {optimal_strategy[:50]}...")
        else:
            print(f"   æœ€ä¼˜ç­–ç•¥: {optimal_strategy}")

        risk_plan = final_summary.get("risk_diversification_plan", "")
        if risk_plan and len(risk_plan) > 60:
            print(f"   é£é™©åˆ†æ•£: {risk_plan[:60]}...")
        elif risk_plan:
            print(f"   é£é™©åˆ†æ•£: {risk_plan}")

        print("=" * 80)

    def save_algorithm_results(self, result: dict):
        """
        ä¿å­˜ç®—æ³•æ¨¡æ‹Ÿç»“æœ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"dlt_detailed_analysis_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filename}")
            return {"status": "success", "filename": filename}
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}


# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–åˆ†æå™¨
    print("ğŸš€ åˆå§‹åŒ–å¤§ä¹é€é«˜çº§åˆ†æå™¨...")
    analyzer = DltAdvancedAnalyzer()

    # æ‰§è¡Œç»¼åˆåˆ†æ
    print("\n" + "=" * 60)
    final_result = analyzer.analyze_dlt_comprehensive()

    # è¿”å›æœ€ç»ˆç»“æœ
    print("\nğŸ“‹ æœ€ç»ˆæ‰§è¡Œç»“æœæ±‡æ€»:")
    print(f"   æ‰§è¡ŒçŠ¶æ€: {final_result.get('status', 'unknown')}")
    if final_result.get('status') == 'success':
        print(f"   é¢„æµ‹æœŸå·: {final_result.get('prediction_period')}")
        print(f"   ä¿å­˜çŠ¶æ€: {final_result.get('save_status', {}).get('status')}")
        print(f"   æ–‡ä»¶è·¯å¾„: {final_result.get('save_status', {}).get('filename')}")
    else:
        print(f"   é”™è¯¯ä¿¡æ¯: {final_result.get('message')}")
        print(f"   è¯¦ç»†é”™è¯¯: {final_result.get('details')}")

    return final_result


if __name__ == "__main__":
    main()