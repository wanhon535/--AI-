# main.py
from src.config.database_config import DatabaseConfig

from src.config.system_config import SystemConfig
from src.database.database_manager import DatabaseManager
from src.engine.recommendation_engine import RecommendationEngine
from src.engine.evaluation_system import EvaluationSystem
from src.algorithms.statistical_algorithms import (
    FrequencyAnalysisAlgorithm,
    HotColdNumberAlgorithm,
    OmissionValueAlgorithm
)
import logging

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æ—¥å¿—
    # main.py (ä½äºé¡¹ç›®æ ¹ç›®å½•)

    import json
    import os
    import sys

    # --- 1. é¡¹ç›®ç¯å¢ƒè®¾ç½® (Setup Project Environment) ---
    # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonçš„æ¨¡å—æœç´¢è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥srcä¸‹çš„æ‰€æœ‰æ¨¡å—
    project_root = os.path.abspath(os.path.dirname(__file__))
    sys.path.insert(0, project_root)
    print(f"Project root added to path: {project_root}")

    # --- 2. å¯¼å…¥é¡¹ç›®æ ¸å¿ƒæ¨¡å— (Import Core Modules) ---
    from src.database.database_manager import DatabaseManager
    from src.prompt_templates import build_lotto_pro_prompt_v14_omega
    from src.llm.clients import get_llm_client

    # --- 3. ä¸»æ‰§è¡Œå‡½æ•° (Main Execution Function) ---
    def run_prediction_pipeline():
        """
        æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ç«¯åˆ°ç«¯é¢„æµ‹æµç¨‹ï¼š
        è¿æ¥æ•°æ®åº“ -> è·å–æ•°æ® -> æ„å»ºPrompt -> è°ƒç”¨AI -> è§£æç»“æœ -> å­˜å…¥æ•°æ®åº“
        """
        print("\n" + "=" * 60)
        print("ğŸ”¥  STARTING LOTTO-PRO PREDICTION PIPELINE (Prometheus-Î© Engine)")
        print("=" * 60)

        # --- åˆå§‹åŒ–å¹¶è¿æ¥æ•°æ®åº“ ---
        db_manager = DatabaseManager(
            host='localhost', user='root', password='123456789',
            database='lottery_analysis_system', port=3309
        )
        if not db_manager.connect():
            print("âŒ CRITICAL: Database connection failed. Aborting pipeline.")
            return

        try:
            # --- æ­¥éª¤ä¸€ï¼šä»æ•°æ®åº“è·å–å®æ—¶æ•°æ® ---
            print("\n[PHASE 1/5] Fetching data from database...")

            # !! æ³¨æ„ï¼šä»¥ä¸‹æ˜¯æˆ‘æ ¹æ®æ‚¨çš„é¡¹ç›®ç»“æ„æ¨æ–­çš„æ–¹æ³•å !!
            # !! å¦‚æœæ‚¨çš„ DatabaseManager ä¸­æ–¹æ³•åä¸åŒï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹ !!
            recent_draws = db_manager.get_latest_lottery_history(50)
            # å‡è®¾æ‚¨æœ‰æ–¹æ³•è·å–ç®—æ³•çš„å†å²è¡¨ç°å’Œä¸Šä¸€æœŸå¤ç›˜æŠ¥å‘Š
            # å¦‚æœæ²¡æœ‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Noneæˆ–ç©ºå­—å…¸ä½œä¸ºå ä½ç¬¦
            performance_log = db_manager.get_performance_log() if hasattr(db_manager, 'get_performance_log') else {}
            last_performance_report = db_manager.get_last_performance_report() if hasattr(db_manager,
                                                                                          'get_last_performance_report') else None
            next_issue = db_manager.get_next_period_number()

            print(f"âœ… Data fetched successfully for next issue: {next_issue}")
            print(f"  - Loaded {len(recent_draws)} recent draws.")
            print(f"  - Loaded {len(performance_log)} performance records.")

            # --- æ­¥éª¤äºŒï¼šæ„å»º Prometheus-Î© Prompt ---
            print("\n[PHASE 2/5] Building Prometheus-Î© prompt...")

            # å‡è®¾æ‚¨çš„ model_outputs æ¥è‡ªäºä¸€ä¸ªç®—æ³•å¼•æ“
            # åœ¨æœ¬æ¬¡æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„ placeholder
            mock_model_outputs = {
                'bayesian': {'status': 'ok'}, 'markov': {'status': 'ok'}, 'graph': {'status': 'ok'},
                'neural': {'status': 'ok'}, 'hit_optimizer': {'status': 'ok'}, 'ensemble': {'status': 'ok'}
            }

            prompt_text, _ = build_lotto_pro_prompt_v14_omega(
                recent_draws=recent_draws,
                model_outputs=mock_model_outputs,
                performance_log=performance_log,
                last_performance_report=last_performance_report,
                next_issue_hint=next_issue,
                risk_preference="å¹³è¡¡"  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            )
            print("âœ… Prompt built successfully.")

            # --- æ­¥éª¤ä¸‰ï¼šè°ƒç”¨é€šä¹‰åƒé—®æ¨¡å‹ ---
            print("\n[PHASE 3/5] Calling Tongyi LLM (qwen3-max)...")

            # è¿™æ˜¯æ‚¨å¯ä»¥è½»æ¾åˆ‡æ¢æ¨¡å‹çš„åœ°æ–¹
            MODEL_TO_USE = "qwen3-max"

            llm_client = get_llm_client(MODEL_TO_USE)
            response_str = llm_client.generate(
                system_prompt=prompt_text,
                user_prompt="è¯·æ ¹æ®ä»¥ä¸ŠæŒ‡ä»¤ï¼Œæ‰§è¡Œä½ çš„åˆ†æå¹¶è¿”å›å®Œæ•´çš„JSONå¯¹è±¡ã€‚"
            )
            print("âœ… LLM response received.")

            # --- æ­¥éª¤å››ï¼šè§£æAIè¿”å›çš„JSONç»“æœ ---
            print("\n[PHASE 4/5] Parsing AI response...")
            try:
                response_data = json.loads(response_str)
                # æå–æ ¸å¿ƒçš„æ¨èåˆ—è¡¨
                recommendations = response_data['cognitive_cycle_outputs']['phase4_portfolio_construction'][
                    'recommendations']
                print(f"âœ… AI response parsed successfully. Found {len(recommendations)} recommendations.")
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âŒ CRITICAL: Failed to parse AI response. Error: {e}")
                print("--- RAW AI RESPONSE ---")
                print(response_str)
                print("-----------------------")
                # å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶ä»¥ä¾¿åˆ†æ
                with open("error_response.log", "w", encoding="utf-8") as f:
                    f.write(response_str)
                print("Raw response saved to error_response.log. Aborting pipeline.")
                return

            # --- æ­¥éª¤äº”ï¼šå°†ç»“æœå­˜å…¥æ•°æ®åº“ ---
            print("\n[PHASE 5/5] Saving recommendations to database...")

            # 1. æ’å…¥æ¨èä¸»è®°å½•
            root_id = db_manager.insert_algorithm_recommendation_root(
                period_number=next_issue,
                model_name=MODEL_TO_USE,
                confidence_score=response_data.get('final_summary', {}).get('confidence_level', 0.85),
                risk_level=response_data.get('final_summary', {}).get('risk_assessment', 'medium')
            )

            if not root_id:
                print("âŒ CRITICAL: Failed to insert recommendation root record. Aborting save.")
                return

            print(f"âœ… Recommendation root record inserted. ID: {root_id}")

            # 2. è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…DAOçš„æ‰¹é‡æ’å…¥æ–¹æ³•
            details_to_insert = []
            for rec in recommendations:
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                front_str = ','.join(map(str, rec.get('front_numbers', [])))
                back_str = ','.join(map(str, rec.get('back_numbers', [])))

                details_to_insert.append({
                    "recommend_type": rec.get('type', 'æœªçŸ¥'),
                    "strategy_logic": rec.get('role_in_portfolio', ''),
                    "front_numbers": front_str,
                    "back_numbers": back_str,
                    "win_probability": rec.get('confidence_score', 0.0)  # å¤ç”¨å­—æ®µ
                })

            # 3. æ‰¹é‡æ’å…¥æ¨èè¯¦æƒ…
            success = db_manager.insert_recommendation_details_batch(
                recommendation_id=root_id,
                details=details_to_insert
            )

            if success:
                print("âœ… Recommendation details inserted successfully.")
            else:
                print("âŒ FAILED: Could not insert recommendation details.")

        except Exception as e:
            print(f"\nâŒ An unexpected error occurred in the pipeline: {e}")
        finally:
            # --- æ–­å¼€æ•°æ®åº“è¿æ¥ ---
            db_manager.disconnect()
            print("\n" + "=" * 60)
            print("ğŸ  PIPELINE FINISHED. Database connection closed.")
            print("=" * 60)

    # --- å¯åŠ¨ç®¡é“ ---
    if __name__ == "__main__":
        run_prediction_pipeline()

if __name__ == "__main__":
    main()
